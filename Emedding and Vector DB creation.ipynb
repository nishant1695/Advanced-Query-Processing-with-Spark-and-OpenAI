{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import time\n",
    "import os\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "import numpy as np\n",
    "from pyspark.sql import SparkSession\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain.embeddings.sentence_transformer import SentenceTransformerEmbeddings\n",
    "from langchain_community.vectorstores import FAISS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Spark session with environment variables\n",
    "spark = SparkSession.builder.appName(\"TextFileProcessing\")\\\n",
    "    .config(\"spark.executorEnv.HF_HOME\", \"/mnt/yarn/usercache/\")\\\n",
    "    .config(\"spark.executorEnv.TRANSFORMERS_CACHE\", \"/mnt/yarn/usercache/\")\\\n",
    "    .getOrCreate()\n",
    "\n",
    "\n",
    "# Create a list of the files to be read from S3 bucket\n",
    "def list_all_txt_files(bucket, prefix):\n",
    "    \"\"\"List all txt files in the specified S3 bucket and prefix using boto3.\"\"\"\n",
    "    s3_client = boto3.client('s3')\n",
    "    paginator = s3_client.get_paginator('list_objects_v2')\n",
    "    page_iterator = paginator.paginate(Bucket=bucket, Prefix=prefix)\n",
    "\n",
    "    txt_files = []\n",
    "    for page in page_iterator:\n",
    "        if 'Contents' in page:\n",
    "            txt_files.extend(['s3://' + bucket + '/' + item['Key']\n",
    "                              for item in page['Contents']\n",
    "                              if item['Key'].endswith('.txt')])\n",
    "    return txt_files\n",
    "\n",
    "#Reading the text file\n",
    "def read_text_from_s3(bucket, key):\n",
    "    \"\"\"Read text file from S3.\"\"\"\n",
    "    s3_client = boto3.client('s3')\n",
    "    obj = s3_client.get_object(Bucket=bucket, Key=key)\n",
    "    return obj['Body'].read().decode('utf-8')\n",
    "\n",
    "import os\n",
    "import boto3\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_community.vectorstores import FAISS\n",
    "\n",
    "#Creating embedding and adding it to vector database\n",
    "def process_text(text, embeddings_broadcast, path):\n",
    "    \"\"\"Process text to generate embeddings and return a FAISS db.\"\"\"\n",
    "    text_splitter = RecursiveCharacterTextSplitter(chunk_size=1500, chunk_overlap=10)\n",
    "    split_text = text_splitter.split_text(text)\n",
    "    \n",
    "    # Utilize the broadcasted embeddings model\n",
    "    embeddings = embeddings_broadcast.value\n",
    "    faiss = FAISS.from_texts(split_text, embeddings)\n",
    "    \n",
    "    # Define local and S3 paths\n",
    "    local_dir = \"/mnt/yarn/usercache/faiss_index\"\n",
    "    faiss.save_local(local_dir)  # Save the FAISS index locally\n",
    "    \n",
    "    # Remove 's3://bucket_name/' from the path and use the rest as part of the S3 key prefix\n",
    "    relative_path = path.replace('s3://metcs777-term-project/', '').strip()\n",
    "    s3_prefix = f'output/faiss_index/{relative_path}/'\n",
    "\n",
    "    # Upload each file in the directory to S3\n",
    "    bucket_name = 'metcs777-term-project'\n",
    "    s3_client = boto3.client('s3')\n",
    "\n",
    "    for filename in os.listdir(local_dir):\n",
    "        local_path = os.path.join(local_dir, filename)\n",
    "        s3_key = os.path.join(s3_prefix, filename)\n",
    "        with open(local_path, 'rb') as data:\n",
    "            s3_client.put_object(Bucket=bucket_name, Key=s3_key, Body=data)\n",
    "    print(f\"Successfully uploaded FAISS index to s3://{bucket_name}/{s3_prefix}\")\n",
    "\n",
    "#Merge the databases\n",
    "def merge_faiss_dbs(dbs):\n",
    "    \"\"\"Merge multiple FAISS databases into one.\"\"\"\n",
    "    final_db = FAISS()\n",
    "    for db in dbs:\n",
    "        final_db.merge_from(db)\n",
    "    return final_db\n",
    "\n",
    "\n",
    "# Broadcast the embeddings model\n",
    "cache_dir = \"/mnt/yarn/usercache/\"\n",
    "os.environ[\"TRANSFORMERS_CACHE\"] = cache_dir  # HuggingFace cache\n",
    "os.environ[\"SENTENCE_TRANSFORMERS_HOME\"] = cache_dir  # Sentence Transformers cache\n",
    "embeddings_model = SentenceTransformerEmbeddings(model_name=\"all-MiniLM-L6-v2\")\n",
    "embeddings_broadcast = spark.sparkContext.broadcast(embeddings_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define bucket and prefix, then retrieve file paths\n",
    "bucket_name = 'metcs777-term-project'\n",
    "prefix = 'output/'\n",
    "\n",
    "files = list_all_txt_files(bucket_name, prefix)\n",
    "\n",
    "# Read and process files in parallel using Spark\n",
    "rdd = spark.sparkContext.parallelize(files)\n",
    "processed_dbs = rdd.map(lambda path: process_text(read_text_from_s3(bucket_name, path.replace(f's3://{bucket_name}/', '')), embeddings_broadcast,path)).collect()\n",
    "print(\"Databases Created and store in S3\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
